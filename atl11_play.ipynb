{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ATLAS/ICESat-2 Land Ice Height Changes ATL11 Exploratory Data Analysis**\n",
    "\n",
    "Adapted from https://github.com/suzanne64/ATL11/blob/master/intro_to_ATL11.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pointCollection.is2_calendar\n",
    "\n",
    "import dask\n",
    "import dask.array\n",
    "import holoviews as hv\n",
    "import hvplot.dask\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "\n",
    "# import intake\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import pygmt\n",
    "import pyproj\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:34953</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>64</li>\n",
       "  <li><b>Cores: </b>64</li>\n",
       "  <li><b>Memory: </b>201.22 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:34953' processes=64 threads=64, memory=201.22 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = dask.distributed.Client(n_workers=64, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from Zarr\n",
    "\n",
    "Let's start by getting our data and running some preprocessing steps:\n",
    "- Load 1385 (reference ground tracks) ATL11/*.zarr files\n",
    "- Convert coordinates from longitude/latitude to x/y\n",
    "- Convert GPS delta_time to UTC time\n",
    "- Mask out low quality height (h_corr) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1385 reference ground track Zarr stores\n"
     ]
    }
   ],
   "source": [
    "stores = glob.glob(pathname=\"ATL11.001z123/ATL11_*.zarr\")\n",
    "print(f\"{len(stores)} reference ground track Zarr stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(\n",
    "    paths=stores,\n",
    "    group=\"pt123\",\n",
    "    engine=\"zarr\",\n",
    "    combine=\"nested\",\n",
    "    concat_dim=\"ref_pt\",\n",
    "    backend_kwargs={\"consolidated\": True},\n",
    ")\n",
    "# ds = ds.unify_chunks().compute()\n",
    "# TODO use intake\n",
    "# source = intake.open_ndzarr(url=\"ATL11.001z123/ATL11_0*.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert geographic lon/lat to x/y\n",
    "\n",
    "To center our plot on the South Pole,\n",
    "we'll reproject the original longitude/latitude coordinates\n",
    "to the Antarctic Polar Stereographic (EPSG:3031) projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "lonlat_to_xy = lambda longitude, latitude: pyproj.Proj(projparams=3031)(\n",
    "    longitude, latitude\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "x, y = lonlat_to_xy(ds.longitude.values, ds.latitude.values)\n",
    "ds[\"x\"] = xr.DataArray(data=x, coords=ds.longitude.coords)\n",
    "ds[\"y\"] = xr.DataArray(data=y, coords=ds.latitude.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Also set x, y as coordinates in xarray.Dataset\n",
    "ds = ds.set_coords(names=[\"x\", \"y\"])\n",
    "# ds = ds.set_index(x=\"x\", y=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert delta_time to utc_time\n",
    "\n",
    "To get more human-readable datetimes,\n",
    "we'll convert the delta_time attribute from the original GPS time format\n",
    "(nanoseconds since the beginning of ICESat-2 starting epoch)\n",
    "to Coordinated Universal Time (UTC).\n",
    "The reference date for the ICESat-2 Epoch is 2018 January 1st according to\n",
    "https://github.com/SmithB/pointCollection/blob/master/is2_calendar.py#L11-L15\n",
    "\n",
    "TODO: Account for [leap seconds](https://en.wikipedia.org/wiki/Leap_second)\n",
    "in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICESAT2_EPOCH = np.datetime64(pointCollection.is2_calendar.t_0())\n",
    "# ICESAT2_EPOCH = np.datetime64(datetime.datetime(2018, 1, 1, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc_time = ICESAT2_EPOCH + ds.delta_time.values\n",
    "ds[\"utc_time\"] = xr.DataArray(data=utc_time, coords=ds.delta_time.coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask out low quality height data\n",
    "\n",
    "Good quality data has value 0, not so good is 1-8.\n",
    "Look at the 'quality_summary_ref_surf' attribute in `ds`\n",
    "for more information on what the quality flags mean.\n",
    "\n",
    "We'll mask out values other than 0 with NaN using xarray's\n",
    "[where](http://xarray.pydata.org/en/v0.15.1/indexing.html#masking-with-where)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"h_corr\"] = ds.h_corr.where(cond=ds.quality_summary_ref_surf == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot into a pandas/dask dataframe\n",
    "\n",
    "To make data analysis and plotting easier,\n",
    "let's flatten our n-dimensional `xarray.Dataset`\n",
    "to a 2-dimensiontal `pandas.DataFrame` table format.\n",
    "\n",
    "There are currently 6 cycles (as of March 2020),\n",
    "and by selecting just one cycle at a time,\n",
    "we can see what the height (`h_corr`)\n",
    "of the ice is like at that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at ICESat-2 Cycle 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_number: int = 6\n",
    "# Subset to essential columns\n",
    "essential_columns = [\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"utc_time\",\n",
    "    \"h_corr\",\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"delta_time\",\n",
    "    \"cycle_number\",\n",
    "]\n",
    "dss = ds.sel(cycle_number=cycle_number)[[*essential_columns]]\n",
    "dss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = hv.Points(\n",
    "    data=dss,\n",
    "    label=f\"Cycle_{cycle_number}\",\n",
    "    kdims=[\"x\", \"y\"],\n",
    "    vdims=[\"utc_time\", \"h_corr\", \"cycle_number\"],\n",
    "    datatype=[\"xarray\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39721113\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>utc_time</th>\n",
       "      <th>h_corr</th>\n",
       "      <th>cycle_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98215</th>\n",
       "      <td>875866.803217</td>\n",
       "      <td>-2.191334e+06</td>\n",
       "      <td>2020-02-01 18:03:28.174083225</td>\n",
       "      <td>-52.742803</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98216</th>\n",
       "      <td>875851.241069</td>\n",
       "      <td>-2.191275e+06</td>\n",
       "      <td>2020-02-01 18:03:28.178659812</td>\n",
       "      <td>-52.848337</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98217</th>\n",
       "      <td>875836.625343</td>\n",
       "      <td>-2.191217e+06</td>\n",
       "      <td>2020-02-01 18:03:28.186106972</td>\n",
       "      <td>-52.702757</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98218</th>\n",
       "      <td>875822.042305</td>\n",
       "      <td>-2.191158e+06</td>\n",
       "      <td>2020-02-01 18:03:28.195041671</td>\n",
       "      <td>-52.711233</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98219</th>\n",
       "      <td>875807.459558</td>\n",
       "      <td>-2.191099e+06</td>\n",
       "      <td>2020-02-01 18:03:28.204009302</td>\n",
       "      <td>-52.781705</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   x             y                      utc_time     h_corr  \\\n",
       "98215  875866.803217 -2.191334e+06 2020-02-01 18:03:28.174083225 -52.742803   \n",
       "98216  875851.241069 -2.191275e+06 2020-02-01 18:03:28.178659812 -52.848337   \n",
       "98217  875836.625343 -2.191217e+06 2020-02-01 18:03:28.186106972 -52.702757   \n",
       "98218  875822.042305 -2.191158e+06 2020-02-01 18:03:28.195041671 -52.711233   \n",
       "98219  875807.459558 -2.191099e+06 2020-02-01 18:03:28.204009302 -52.781705   \n",
       "\n",
       "       cycle_number  \n",
       "98215             6  \n",
       "98216             6  \n",
       "98217             6  \n",
       "98218             6  \n",
       "98219             6  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = points.dframe()  # convert to pandas.DataFrame, slow\n",
    "df = df.dropna()  # drop empty rows\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a sample of the points over Antarctica\n",
    "\n",
    "Let's take a look at an interactive map\n",
    "of the ICESat-2 ATL11 height for Cycle 6!\n",
    "We'll plot a random sample (n=5 million)\n",
    "of the points instead of the whole dataset,\n",
    "it should give a good enough picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=5_000_000).hvplot.points(\n",
    "    title=f\"Elevation (metres) at Cycle {cycle_number}\",\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    c=\"h_corr\",\n",
    "    cmap=\"Blues\",\n",
    "    rasterize=True,\n",
    "    hover=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset to geographic region of interest (optional)\n",
    "\n",
    "Take a geographical subset and save to a NetCDF/Zarr format for distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kamb Ice Stream bounding box in EPSG:3031 coordinates\n",
    "xmin, xmax, ymin, ymax = (\n",
    "    -739741.7702261859,\n",
    "    -411054.19240523444,\n",
    "    -699564.516934089,\n",
    "    -365489.6822096751,\n",
    ")\n",
    "cond = xr.ufuncs.logical_and(\n",
    "    xr.ufuncs.logical_and(ds.x > xmin, ds.x < xmax),\n",
    "    xr.ufuncs.logical_and(ds.y > ymin, ds.y < ymax),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the actual computation to find data points within region of interest\n",
    "ds_subset = ds.where(cond=cond, drop=True)\n",
    "ds_subset = ds_subset.unify_chunks()\n",
    "ds_subset = ds_subset.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7fac1e77ac20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to NetCDF/Zarr formats for distribution\n",
    "ds_subset.to_netcdf(\n",
    "    path=\"atl11_subset.nc\", engine=\"h5netcdf\",\n",
    ")\n",
    "ds_subset.to_zarr(\n",
    "    store=\"atl11_subset.zarr\", mode=\"w\", consolidated=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Cycle Number 6 only for plotting\n",
    "points_subset = hv.Points(\n",
    "    data=ds_subset.sel(cycle_number=6)[[*essential_columns]],\n",
    "    label=\"Cycle_6\",\n",
    "    kdims=[\"x\", \"y\"],\n",
    "    vdims=[\"utc_time\", \"h_corr\", \"cycle_number\"],\n",
    "    datatype=[\"xarray\"],\n",
    ")\n",
    "df_subset = points_subset.dframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot our subset of points on an interactive map\n",
    "df_subset.hvplot.points(\n",
    "    title=f\"Elevation (metres) at Cycle {cycle_number}\",\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    c=\"h_corr\",\n",
    "    cmap=\"Blues\",\n",
    "    rasterize=True,\n",
    "    hover=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "sorted(os.listdir(\"ATL11.001/\"))\n",
    "thefile = \"ATL11.001/ATL11_076211_0104_02_v001.h5\"\n",
    "\n",
    "with h5py.File(thefile, mode=\"r\") as h5f:\n",
    "    print(h5f.keys())\n",
    "    print(h5f[\"pt1\"].keys())\n",
    "\n",
    "with xr.open_dataset(thefile, group=\"pt1/ref_surf\", engine=\"h5netcdf\") as rs:\n",
    "    # read in the along-track coordinates\n",
    "    x_atc = rs.x_atc.to_masked_array()\n",
    "    # N slope\n",
    "    n_slope = rs.n_slope.to_masked_array()\n",
    "    e_slope = rs.e_slope.to_masked_array()\n",
    "    bad = n_slope == 1.7976931348623157e308\n",
    "    bad |= e_slope == 1.7976931348623157e308\n",
    "    n_slope[bad] = np.NaN\n",
    "    e_slope[bad] = np.NaN\n",
    "    slope_mag = np.sqrt(n_slope ** 2 + e_slope ** 2)\n",
    "    # get the reference-surface quality summary\n",
    "    r_quality_summary = rs.quality_summary.to_masked_array()\n",
    "\n",
    "with xr.open_dataset(thefile, group=\"pt1/corrected_h\", engine=\"h5netcdf\") as ch:\n",
    "    # read in the along-track coordinates\n",
    "    ref_pt = ch.ref_pt.to_masked_array()\n",
    "    # read in the corrected_h\n",
    "    h_corr = ch.h_corr.to_masked_array()\n",
    "    # mask out invalid values\n",
    "    bad = h_corr == 1.7976931348623157e308\n",
    "    h_corr[bad] = np.NaN\n",
    "\n",
    "    # error\n",
    "    h_corr_sigma = ch.h_corr_sigma.to_masked_array()\n",
    "    bad = h_corr_sigma == 1.7976931348623157e308\n",
    "    h_corr_sigma[bad] = np.NaN\n",
    "    # systematic error\n",
    "    h_corr_sigma_s = ch.h_corr_sigma_systematic.to_masked_array()\n",
    "    bad = h_corr_sigma == 1.7976931348623157e308\n",
    "    h_corr_sigma_s[bad] = np.NaN\n",
    "    # get the ATL06-based quality summary\n",
    "    h_quality_summary = ch.quality_summary.to_masked_array()\n",
    "    # read the cycle_number\n",
    "    cycle_num = ch.cycle_number.to_masked_array()\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    plt.figure()\n",
    "    plt.subplot(211)\n",
    "    for cycle in range(0, h_corr.shape[1]):\n",
    "        plt.plot(x_atc, h_corr[:, cycle], \".\", label=f\"cycle {cycle}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"along-track distance\")\n",
    "    plt.ylabel(\"height, m\")\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(x_atc, np.sum(np.isfinite(h_corr), axis=1), \".\")\n",
    "    plt.xlabel(\"along-track x\")\n",
    "    plt.ylabel(\"number of cycles present\")\n",
    "\n",
    "x_rep = np.tile(x_atc[:, np.newaxis], [1, len(cycle_num)])\n",
    "q_rep = np.tile(r_quality_summary[:, np.newaxis], [1, len(cycle_num)])\n",
    "\n",
    "if 2 == 2:\n",
    "    plt.figure()\n",
    "    plt.plot(x_atc, r_quality_summary, \".\")\n",
    "    plt.xlabel(\"x_atc\")\n",
    "    plt.ylabel(\"reference-surface quality summary\")\n",
    "\n",
    "\n",
    "if 3 == 3:\n",
    "    fig = plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.plot(\n",
    "        x_rep.ravel()[q_rep.ravel() != 6], h_corr.ravel()[q_rep.ravel() != 6], \"k.\"\n",
    "    )\n",
    "    plt.title(\"points with surface quality not equal to 6\")\n",
    "    # plt.gca().set_xlim([6.75e6, 6.87e6])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 4 == 4:\n",
    "    comblist = list(itertools.combinations(cycle_num, 2))\n",
    "    plt.figure(len(comblist) + 1, figsize=(4, 2 * len(comblist) + 1))\n",
    "    plt.clf()\n",
    "    ax = []\n",
    "    good = np.flatnonzero(q_rep[:, 0] != 6)\n",
    "\n",
    "    # cycle-to-cycle elevation differences\n",
    "    for j, (col1, col2) in enumerate(comblist, start=1):\n",
    "        ax += [plt.subplot(len(comblist) + 1, 1, j)]\n",
    "        col1 -= 1\n",
    "        col2 -= 1\n",
    "\n",
    "        this_dh = h_corr[good, col2] - h_corr[good, col1]\n",
    "        # cycle-to-cycle difference errors are the quadratic sums of the cycle errors\n",
    "        this_dh_sigma = np.sqrt(\n",
    "            h_corr_sigma[good, col2] ** 2 + h_corr_sigma[good, col1] ** 2\n",
    "        )\n",
    "        # Likewise for systematic errors:\n",
    "        this_dh_sigma_s = np.sqrt(\n",
    "            h_corr_sigma_s[good, col2] ** 2 + h_corr_sigma_s[good, col1] ** 2\n",
    "        )\n",
    "        plt.errorbar(\n",
    "            x_rep[good, col2].ravel(),\n",
    "            this_dh,\n",
    "            yerr=np.sqrt(this_dh_sigma ** 2 + this_dh_sigma_s ** 2),\n",
    "            fmt=\"r.\",\n",
    "        )\n",
    "        plt.errorbar(x_rep[good, col2].ravel(), this_dh, yerr=this_dh_sigma, fmt=\"k.\")\n",
    "        ax[-1].set_ylabel(f\"cycle {col2+1} \\n minus \\n cycle {col1+1}\")\n",
    "        ax[-1].set_ylim([-5, 5])\n",
    "\n",
    "    # plot of the number of cycles available:\n",
    "    ax += [plt.subplot(len(comblist) + 1, 1, j + 1)]\n",
    "    plt.plot(x_atc, np.sum(np.isfinite(h_corr) & (q_rep != 6), axis=1), \".\")\n",
    "    ax[-1].set_ylabel(\"# of cycles available\")\n",
    "    ax[-1].set_ylim([0, 3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "dh (change in elevation) over Antarctica!"
   },
   "outputs": [],
   "source": [
    "def read_field(dataset: xr.Dataset, field: str):\n",
    "    data = dataset[field].to_masked_array()\n",
    "    bad1 = data == 1.7976931348623157e308\n",
    "    data[bad1] = np.NaN\n",
    "    bad2 = data == -1.7976931348623157e308\n",
    "    data[bad2] = np.NaN\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_ATL11(filepath: str, pair: str = \"pt2\", epsg: int = 3031):\n",
    "    with xr.open_mfdataset(\n",
    "        paths=filepath, group=f\"{pair}/corrected_h\", engine=\"h5netcdf\"\n",
    "    ) as ch:\n",
    "        longitude = read_field(dataset=ch, field=\"longitude\")\n",
    "        latitude = read_field(dataset=ch, field=\"latitude\")\n",
    "        h_corr = read_field(dataset=ch, field=\"h_corr\")\n",
    "        h_corr_sigma = read_field(dataset=ch, field=\"h_corr_sigma\")\n",
    "        h_corr_sigma_s = read_field(dataset=ch, field=\"h_corr_sigma_systematic\")\n",
    "    with xr.open_mfdataset(\n",
    "        paths=filepath, group=f\"{pair}/ref_surf\", engine=\"h5netcdf\"\n",
    "    ) as rs:\n",
    "        x_atc = read_field(dataset=rs, field=\"x_atc\")\n",
    "        quality = rs[\"quality_summary\"].to_masked_array().data\n",
    "    h_corr[quality == 6] = np.NaN\n",
    "    x, y = pyproj.Proj(projparams=epsg)(longitude, latitude)\n",
    "    return x_atc, x, y, h_corr, np.sqrt(h_corr_sigma ** 2 + h_corr_sigma_s ** 2)\n",
    "\n",
    "\n",
    "x_atc = []\n",
    "x = []\n",
    "y = []\n",
    "h_corr = []\n",
    "sigma_h = []\n",
    "for pair in [\"pt1\", \"pt2\", \"pt3\"]:\n",
    "    xx_atc, xx, yy, hh, ss = read_ATL11(\n",
    "        filepath=\"ATL11.001/ATL11_????11_0105_02_v001.h5\", pair=pair\n",
    "    )\n",
    "    x_atc += [xx_atc]\n",
    "    x += [xx]\n",
    "    y += [yy]\n",
    "    h_corr += [hh]\n",
    "    sigma_h += [ss]\n",
    "\n",
    "# with xr.open_mfdataset(\n",
    "#     paths=\"ATL11.001/ATL11_????11_0105_02_v001.h5\",\n",
    "#     group=f\"{pair}/corrected_h\",\n",
    "#     engine=\"h5netcdf\",\n",
    "#     lock=False,\n",
    "#     # combine=\"nested\",\n",
    "#     # concat_dim=None,\n",
    "#     # drop_variables=\"delta_time\"\n",
    "# ) as ch:\n",
    "#     pass\n",
    "\n",
    "x_atc = np.concatenate(x_atc)\n",
    "x = np.concatenate(x)\n",
    "y = np.concatenate(y)\n",
    "h_corr = np.concatenate(h_corr, axis=0)\n",
    "sigma_h = np.concatenate(sigma_h, axis=0)\n",
    "\n",
    "print(h_corr.shape)\n",
    "\n",
    "if 5 == 5:\n",
    "    c2: int = 5\n",
    "    c1: int = 4\n",
    "    plt.figure(figsize=[10, 10])\n",
    "    plt.scatter(\n",
    "        x=x[::20],\n",
    "        y=y[::20],\n",
    "        s=2,\n",
    "        c=h_corr[::20, c2 - 1] - h_corr[::20, c1 - 1],\n",
    "        vmin=-2,\n",
    "        vmax=2,\n",
    "        cmap=\"Spectral\",\n",
    "    )\n",
    "    plt.axis(\"equal\")\n",
    "    hb = plt.colorbar()\n",
    "    hb.set_label(f\"cycle {c2} minus cycle {c1} elevation change (dh) in metres\")\n",
    "\n",
    "# TODO https://github.com/ICESAT-2HackWeek/elevation-change/blob/master/elevation_change_with_ATL11.ipynb\n",
    "\n",
    "sdf = pd.DataFrame(sigma_h, columns=[f\"s{i + 1}\" for i in range(5)])\n",
    "\n",
    "df = pd.concat(\n",
    "    objs=[\n",
    "        pd.DataFrame(data=x_atc, columns=[\"x_atc\"]),\n",
    "        pd.DataFrame(data=x, columns=[\"x\"]),\n",
    "        pd.DataFrame(data=y, columns=[\"y\"]),\n",
    "        pd.DataFrame(data=h_corr, columns=[f\"h{i + 1}\" for i in range(5)]),\n",
    "    ],\n",
    "    axis=\"columns\",\n",
    ")\n",
    "df.head()\n",
    "\n",
    "# TODO range of dh along window view of point with big change\n",
    "\n",
    "# Cycle 1 - Spring2018 - 13Oct2018 - 28Dec2018  -ve MassBalance\n",
    "# Cycle 2 - Summer2019 - 28Dec2018 - 29Mar2019 --ve MassBalance\n",
    "# Cycle 3 - Autumn2019 - 29Mar2019 - 28Jun2019  +ve MassBalance *\n",
    "# Cycle 4 - Winter2019 - 09Jul2019 - 26Sep2019 ++ve MassBalance *\n",
    "# Cycle 5 - Spring2019 - 26Sep2019 - 26Dec2019  -ve MassBalance *\n",
    "# Cycle 6 - Summer2020 - 26Dec2019 - 26Mar2020 --ve MassBalance\n",
    "\n",
    "hmin = df[[f\"h{i+1}\" for i in range(5)]].min(axis=\"columns\")  # minimum elevation\n",
    "hmax = df[[f\"h{i+1}\" for i in range(5)]].max(axis=\"columns\")  # maximum elevation\n",
    "df[\"hrange\"] = hmax - hmin  # range of elevation across all cycles\n",
    "df.hrange.replace(to_replace=0.0, value=np.NaN, inplace=True)\n",
    "df.to_csv(\"xyhr.csv\")\n",
    "# df = pd.read_csv(\"xyhr.csv\", index_col=0)\n",
    "bigdh = df[df[\"hrange\"] > 5.5]  # find points where elevation range is greater than 5.5m\n",
    "bigdh\n",
    "bigdh.index\n",
    "\n",
    "# TODO point in polygon (grounding line) to filter out ice shelf dynamics\n",
    "\n",
    "for i in bigdh.index[:]:\n",
    "    # i = 4848718\n",
    "    temp_df = df.loc[i - 10 : i + 10]\n",
    "    median_change = temp_df.hrange.median()\n",
    "    if median_change >= 5.5 and median_change < 50:\n",
    "        temp_sdf = sdf.loc[i - 10 : i + 10]\n",
    "        for j in range(5):\n",
    "            plt.errorbar(\n",
    "                x=temp_df.x_atc,\n",
    "                y=temp_df[f\"h{j+1}\"],\n",
    "                yerr=temp_sdf[f\"s{j+1}\"],\n",
    "                fmt=\"k.\",\n",
    "            )\n",
    "            plt.scatter(x=temp_df.x_atc, y=temp_df[f\"h{j+1}\"], label=f\"h{j+1}\")\n",
    "        plt.title(\n",
    "            label=f\"xy:{temp_df.loc[i].x},{temp_df.loc[i].y}\\nindex:{i}, median_change:{median_change}m\"\n",
    "        )\n",
    "\n",
    "        plt.gca().set_xlim(temp_df.x_atc[i - 10], temp_df.x_atc[i + 10])\n",
    "        # plt.gca().set_ylim(160, 200)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Subglacial lake Slessor2 uplift\n",
    "# -410918.8386,1029347.4666\n",
    "# -408131.9125,1031128.9651\n",
    "# Subglacial Lake Slessor4 drain\n",
    "# -338117.9641,1110603.6373\n",
    "\n",
    "# Subglacial Lake Whillans4/Mercer2 drainage\n",
    "# -307154.8016,-507734.7378\n",
    "\n",
    "# Subglacial Lake Macayeal 3 drainage (manually found)\n",
    "# -734532.7023, -855436.2967\n",
    "\n",
    "# Subglacial Lake Byrd 2 uplift (manually found, ~2m)\n",
    "# 557187.1725,-855601.0561\n",
    "# 555843.4189,-985710.3337 # Upstream Byrd Glacier ? rifting ??\n",
    "\n",
    "# -741220.3139, 937483.8670 # Ronne-Filchner Ice Shelf\n",
    "# -973351.7558, 272566.6157 # Ronne-Filchner Ice Shelf\n",
    "# -1008445.1929,274272.3455  # Ronne-Filchner Ice Shelf\n",
    "# 37261.1917,-1180880.8635 Ross Sea tidal motion\n",
    "# -579964.1805,574791.5220 # Support Force Glacier at grounding line\n",
    "# -1174079.4108,212533.0448 # Rutford Ice Stream/Shelf tidal motion?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "deepicedrain",
   "language": "python",
   "name": "deepicedrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
