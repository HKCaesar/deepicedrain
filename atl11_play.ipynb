{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/suzanne64/ATL11/blob/master/intro_to_ATL11.ipynb\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import dask\n",
    "import dask.distributed\n",
    "import h5py\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(n_workers=64, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create ATL06_to_ATL11 processing script, if not already present\n",
    "if not os.path.exists(\"ATL06_to_ATL11_Antarctica.sh\"):\n",
    "    # find number of cycles for each reference ground track and each orbital segment\n",
    "    func = lambda ref_gt, orb_st: len(\n",
    "        glob.glob(f\"ATL06.003/**/ATL06*_*_{ref_gt:04d}??{orb_st}_*.h5\")\n",
    "    )\n",
    "    futures = []\n",
    "    for referencegroundtrack in range(1387, 0, -1):\n",
    "        for orbitalsegment in [10, 11, 12]:  # loop through Antarctic orbital segments\n",
    "            numcycles = client.submit(\n",
    "                func,\n",
    "                referencegroundtrack,\n",
    "                orbitalsegment,\n",
    "                key=f\"{referencegroundtrack:04d}-{orbitalsegment}\",\n",
    "            )\n",
    "            futures.append(numcycles)\n",
    "\n",
    "    # Prepare string to write into ATL06_to_ATL11_Antarctica.sh bash script\n",
    "    writelines = []\n",
    "    for f in tqdm.tqdm(\n",
    "        iterable=dask.distributed.as_completed(futures=futures), total=len(futures)\n",
    "    ):\n",
    "        referencegroundtrack, orbitalsegment = f.key.split(\"-\")\n",
    "        cycles = f.result()\n",
    "        writelines.append(\n",
    "            f\"python3 ATL11/ATL06_to_ATL11.py\"\n",
    "            f\" {referencegroundtrack} {orbitalsegment}\"\n",
    "            f\" --cycles 01 {cycles:02d}\"\n",
    "            f\" --Release 3\"\n",
    "            f\" --directory 'ATL06.003/**/'\"\n",
    "            f\" --out_dir ATL11.001\\n\",\n",
    "        )\n",
    "    writelines.sort()  # sort writelines in place\n",
    "\n",
    "    # Finally create the bash script\n",
    "    with open(file=\"ATL06_to_ATL11_Antarctica.sh\", mode=\"w\") as f:\n",
    "        f.writelines(writelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Now use GNU parallel to run the script, command as below:\n",
    "# !parallel --jobs 20 < ATL06_to_ATL11_Antarctica.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "sorted(os.listdir(\"ATL11.001/\"))\n",
    "thefile = \"ATL11.001/ATL11_076211_0104_02_v001.h5\"\n",
    "\n",
    "with h5py.File(thefile, mode=\"r\") as h5f:\n",
    "    print(h5f.keys())\n",
    "    print(h5f[\"pt1\"].keys())\n",
    "\n",
    "with xr.open_dataset(thefile, group=\"pt1/ref_surf\", engine=\"h5netcdf\") as rs:\n",
    "    # read in the along-track coordinates\n",
    "    x_atc = rs.x_atc.to_masked_array()\n",
    "    # N slope\n",
    "    n_slope = rs.n_slope.to_masked_array()\n",
    "    e_slope = rs.e_slope.to_masked_array()\n",
    "    bad = n_slope == 1.7976931348623157e308\n",
    "    bad |= e_slope == 1.7976931348623157e308\n",
    "    n_slope[bad] = np.NaN\n",
    "    e_slope[bad] = np.NaN\n",
    "    slope_mag = np.sqrt(n_slope ** 2 + e_slope ** 2)\n",
    "    # get the reference-surface quality summary\n",
    "    r_quality_summary = rs.quality_summary.to_masked_array()\n",
    "\n",
    "with xr.open_dataset(thefile, group=\"pt1/corrected_h\", engine=\"h5netcdf\") as ch:\n",
    "    # read in the along-track coordinates\n",
    "    ref_pt = ch.ref_pt.to_masked_array()\n",
    "    # read in the corrected_h\n",
    "    h_corr = ch.h_corr.to_masked_array()\n",
    "    # mask out invalid values\n",
    "    bad = h_corr == 1.7976931348623157e308\n",
    "    h_corr[bad] = np.NaN\n",
    "\n",
    "    # error\n",
    "    h_corr_sigma = ch.h_corr_sigma.to_masked_array()\n",
    "    bad = h_corr_sigma == 1.7976931348623157e308\n",
    "    h_corr_sigma[bad] = np.NaN\n",
    "    # systematic error\n",
    "    h_corr_sigma_s = ch.h_corr_sigma_systematic.to_masked_array()\n",
    "    bad = h_corr_sigma == 1.7976931348623157e308\n",
    "    h_corr_sigma_s[bad] = np.NaN\n",
    "    # get the ATL06-based quality summary\n",
    "    h_quality_summary = ch.quality_summary.to_masked_array()\n",
    "    # read the cycle_number\n",
    "    cycle_num = ch.cycle_number.to_masked_array()\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    plt.figure()\n",
    "    plt.subplot(211)\n",
    "    for cycle in range(0, h_corr.shape[1]):\n",
    "        plt.plot(x_atc, h_corr[:, cycle], \".\", label=f\"cycle {cycle}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"along-track distance\")\n",
    "    plt.ylabel(\"height, m\")\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(x_atc, np.sum(np.isfinite(h_corr), axis=1), \".\")\n",
    "    plt.xlabel(\"along-track x\")\n",
    "    plt.ylabel(\"number of cycles present\")\n",
    "\n",
    "x_rep = np.tile(x_atc[:, np.newaxis], [1, len(cycle_num)])\n",
    "q_rep = np.tile(r_quality_summary[:, np.newaxis], [1, len(cycle_num)])\n",
    "\n",
    "if 2 == 2:\n",
    "    plt.figure()\n",
    "    plt.plot(x_atc, r_quality_summary, \".\")\n",
    "    plt.xlabel(\"x_atc\")\n",
    "    plt.ylabel(\"reference-surface quality summary\")\n",
    "\n",
    "\n",
    "if 3 == 3:\n",
    "    fig = plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.plot(\n",
    "        x_rep.ravel()[q_rep.ravel() != 6], h_corr.ravel()[q_rep.ravel() != 6], \"k.\"\n",
    "    )\n",
    "    plt.title(\"points with surface quality not equal to 6\")\n",
    "    # plt.gca().set_xlim([6.75e6, 6.87e6])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 4 == 4:\n",
    "    comblist = list(itertools.combinations(cycle_num, 2))\n",
    "    plt.figure(len(comblist) + 1, figsize=(4, 2 * len(comblist) + 1))\n",
    "    plt.clf()\n",
    "    ax = []\n",
    "    good = np.flatnonzero(q_rep[:, 0] != 6)\n",
    "\n",
    "    # cycle-to-cycle elevation differences\n",
    "    for j, (col1, col2) in enumerate(comblist, start=1):\n",
    "        ax += [plt.subplot(len(comblist) + 1, 1, j)]\n",
    "        col1 -= 1\n",
    "        col2 -= 1\n",
    "\n",
    "        this_dh = h_corr[good, col2] - h_corr[good, col1]\n",
    "        # cycle-to-cycle difference errors are the quadratic sums of the cycle errors\n",
    "        this_dh_sigma = np.sqrt(\n",
    "            h_corr_sigma[good, col2] ** 2 + h_corr_sigma[good, col1] ** 2\n",
    "        )\n",
    "        # Likewise for systematic errors:\n",
    "        this_dh_sigma_s = np.sqrt(\n",
    "            h_corr_sigma_s[good, col2] ** 2 + h_corr_sigma_s[good, col1] ** 2\n",
    "        )\n",
    "        plt.errorbar(\n",
    "            x_rep[good, col2].ravel(),\n",
    "            this_dh,\n",
    "            yerr=np.sqrt(this_dh_sigma ** 2 + this_dh_sigma_s ** 2),\n",
    "            fmt=\"r.\",\n",
    "        )\n",
    "        plt.errorbar(x_rep[good, col2].ravel(), this_dh, yerr=this_dh_sigma, fmt=\"k.\")\n",
    "        ax[-1].set_ylabel(f\"cycle {col2+1} \\n minus \\n cycle {col1+1}\")\n",
    "        ax[-1].set_ylim([-5, 5])\n",
    "\n",
    "    # plot of the number of cycles available:\n",
    "    ax += [plt.subplot(len(comblist) + 1, 1, j + 1)]\n",
    "    plt.plot(x_atc, np.sum(np.isfinite(h_corr) & (q_rep != 6), axis=1), \".\")\n",
    "    ax[-1].set_ylabel(\"# of cycles available\")\n",
    "    ax[-1].set_ylim([0, 3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "dh (change in elevation) over Antarctica!"
   },
   "outputs": [],
   "source": [
    "def read_field(dataset: xr.Dataset, field: str):\n",
    "    data = dataset[field].to_masked_array()\n",
    "    bad1 = data == 1.7976931348623157e308\n",
    "    data[bad1] = np.NaN\n",
    "    bad2 = data == -1.7976931348623157e308\n",
    "    data[bad2] = np.NaN\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_ATL11(filepath: str, pair: str = \"pt2\", epsg: int = 3031):\n",
    "    with xr.open_mfdataset(\n",
    "        paths=filepath, group=f\"{pair}/corrected_h\", engine=\"h5netcdf\"\n",
    "    ) as ch:\n",
    "        longitude = read_field(dataset=ch, field=\"longitude\")\n",
    "        latitude = read_field(dataset=ch, field=\"latitude\")\n",
    "        h_corr = read_field(dataset=ch, field=\"h_corr\")\n",
    "        h_corr_sigma = read_field(dataset=ch, field=\"h_corr_sigma\")\n",
    "        h_corr_sigma_s = read_field(dataset=ch, field=\"h_corr_sigma_systematic\")\n",
    "    with xr.open_mfdataset(\n",
    "        paths=filepath, group=f\"{pair}/ref_surf\", engine=\"h5netcdf\"\n",
    "    ) as rs:\n",
    "        x_atc = read_field(dataset=rs, field=\"x_atc\")\n",
    "        quality = rs[\"quality_summary\"].to_masked_array().data\n",
    "    h_corr[quality == 6] = np.NaN\n",
    "    x, y = pyproj.Proj(projparams=epsg)(longitude, latitude)\n",
    "    return x_atc, x, y, h_corr, np.sqrt(h_corr_sigma ** 2 + h_corr_sigma_s ** 2)\n",
    "\n",
    "\n",
    "x_atc = []\n",
    "x = []\n",
    "y = []\n",
    "h_corr = []\n",
    "sigma_h = []\n",
    "for pair in [\"pt1\", \"pt2\", \"pt3\"]:\n",
    "    xx_atc, xx, yy, hh, ss = read_ATL11(\n",
    "        filepath=\"ATL11.001/ATL11_????11_0105_02_v001.h5\", pair=pair\n",
    "    )\n",
    "    x_atc += [xx_atc]\n",
    "    x += [xx]\n",
    "    y += [yy]\n",
    "    h_corr += [hh]\n",
    "    sigma_h += [ss]\n",
    "\n",
    "# with xr.open_mfdataset(\n",
    "#     paths=\"ATL11.001/ATL11_????11_0105_02_v001.h5\",\n",
    "#     group=f\"{pair}/corrected_h\",\n",
    "#     engine=\"h5netcdf\",\n",
    "#     lock=False,\n",
    "#     # combine=\"nested\",\n",
    "#     # concat_dim=None,\n",
    "#     # drop_variables=\"delta_time\"\n",
    "# ) as ch:\n",
    "#     pass\n",
    "\n",
    "x_atc = np.concatenate(x_atc)\n",
    "x = np.concatenate(x)\n",
    "y = np.concatenate(y)\n",
    "h_corr = np.concatenate(h_corr, axis=0)\n",
    "sigma_h = np.concatenate(sigma_h, axis=0)\n",
    "\n",
    "print(h_corr.shape)\n",
    "\n",
    "if 5 == 5:\n",
    "    c2: int = 5\n",
    "    c1: int = 4\n",
    "    plt.figure(figsize=[10, 10])\n",
    "    plt.scatter(\n",
    "        x=x[::20],\n",
    "        y=y[::20],\n",
    "        s=2,\n",
    "        c=h_corr[::20, c2 - 1] - h_corr[::20, c1 - 1],\n",
    "        vmin=-2,\n",
    "        vmax=2,\n",
    "        cmap=\"Spectral\",\n",
    "    )\n",
    "    plt.axis(\"equal\")\n",
    "    hb = plt.colorbar()\n",
    "    hb.set_label(f\"cycle {c2} minus cycle {c1} elevation change (dh) in metres\")\n",
    "\n",
    "# TODO https://github.com/ICESAT-2HackWeek/elevation-change/blob/master/elevation_change_with_ATL11.ipynb\n",
    "\n",
    "sdf = pd.DataFrame(sigma_h, columns=[f\"s{i + 1}\" for i in range(5)])\n",
    "\n",
    "df = pd.concat(\n",
    "    objs=[\n",
    "        pd.DataFrame(data=x_atc, columns=[\"x_atc\"]),\n",
    "        pd.DataFrame(data=x, columns=[\"x\"]),\n",
    "        pd.DataFrame(data=y, columns=[\"y\"]),\n",
    "        pd.DataFrame(data=h_corr, columns=[f\"h{i + 1}\" for i in range(5)]),\n",
    "    ],\n",
    "    axis=\"columns\",\n",
    ")\n",
    "df.head()\n",
    "\n",
    "# TODO range of dh along window view of point with big change\n",
    "\n",
    "# Cycle 1 - Spring2018 - 13Oct2018 - 28Dec2018  -ve MassBalance\n",
    "# Cycle 2 - Summer2019 - 28Dec2018 - 29Mar2019 --ve MassBalance\n",
    "# Cycle 3 - Autumn2019 - 29Mar2019 - 28Jun2019  +ve MassBalance *\n",
    "# Cycle 4 - Winter2019 - 09Jul2019 - 26Sep2019 ++ve MassBalance *\n",
    "# Cycle 5 - Spring2019 - 26Sep2019 - 26Dec2019  -ve MassBalance *\n",
    "# Cycle 6 - Summer2020 - 26Dec2019 - 26Mar2020 --ve MassBalance\n",
    "\n",
    "hmin = df[[f\"h{i+1}\" for i in range(5)]].min(axis=\"columns\")  # minimum elevation\n",
    "hmax = df[[f\"h{i+1}\" for i in range(5)]].max(axis=\"columns\")  # maximum elevation\n",
    "df[\"hrange\"] = hmax - hmin  # range of elevation across all cycles\n",
    "df.hrange.replace(to_replace=0.0, value=np.NaN, inplace=True)\n",
    "df.to_csv(\"xyhr.csv\")\n",
    "# df = pd.read_csv(\"xyhr.csv\", index_col=0)\n",
    "bigdh = df[df[\"hrange\"] > 5.5]  # find points where elevation range is greater than 5.5m\n",
    "bigdh\n",
    "bigdh.index\n",
    "\n",
    "# TODO point in polygon (grounding line) to filter out ice shelf dynamics\n",
    "\n",
    "for i in bigdh.index[:]:\n",
    "    # i = 4848718\n",
    "    temp_df = df.loc[i - 10 : i + 10]\n",
    "    median_change = temp_df.hrange.median()\n",
    "    if median_change >= 5.5 and median_change < 50:\n",
    "        temp_sdf = sdf.loc[i - 10 : i + 10]\n",
    "        for j in range(5):\n",
    "            plt.errorbar(\n",
    "                x=temp_df.x_atc,\n",
    "                y=temp_df[f\"h{j+1}\"],\n",
    "                yerr=temp_sdf[f\"s{j+1}\"],\n",
    "                fmt=\"k.\",\n",
    "            )\n",
    "            plt.scatter(x=temp_df.x_atc, y=temp_df[f\"h{j+1}\"], label=f\"h{j+1}\")\n",
    "        plt.title(\n",
    "            label=f\"xy:{temp_df.loc[i].x},{temp_df.loc[i].y}\\nindex:{i}, median_change:{median_change}m\"\n",
    "        )\n",
    "\n",
    "        plt.gca().set_xlim(temp_df.x_atc[i - 10], temp_df.x_atc[i + 10])\n",
    "        # plt.gca().set_ylim(160, 200)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Subglacial lake Slessor2 uplift\n",
    "# -410918.8386,1029347.4666\n",
    "# -408131.9125,1031128.9651\n",
    "# Subglacial Lake Slessor4 drain\n",
    "# -338117.9641,1110603.6373\n",
    "\n",
    "# Subglacial Lake Whillans4/Mercer2 drainage\n",
    "# -307154.8016,-507734.7378\n",
    "\n",
    "# Subglacial Lake Macayeal 3 drainage (manually found)\n",
    "# -734532.7023, -855436.2967\n",
    "\n",
    "# Subglacial Lake Byrd 2 uplift (manually found, ~2m)\n",
    "# 557187.1725,-855601.0561\n",
    "# 555843.4189,-985710.3337 # Upstream Byrd Glacier ? rifting ??\n",
    "\n",
    "# -741220.3139, 937483.8670 # Ronne-Filchner Ice Shelf\n",
    "# -973351.7558, 272566.6157 # Ronne-Filchner Ice Shelf\n",
    "# -1008445.1929,274272.3455  # Ronne-Filchner Ice Shelf\n",
    "# 37261.1917,-1180880.8635 Ross Sea tidal motion\n",
    "# -579964.1805,574791.5220 # Support Force Glacier at grounding line\n",
    "# -1174079.4108,212533.0448 # Rutford Ice Stream/Shelf tidal motion?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "formats": "ipynb,py:hydrogen",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "deepicedrain",
   "language": "python",
   "name": "deepicedrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
