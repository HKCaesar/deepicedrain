{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ATLAS/ICESat-2 Land Ice Height Changes ATL11 Exploratory Data Analysis**\n",
    "\n",
    "Adapted from https://github.com/suzanne64/ATL11/blob/master/intro_to_ATL11.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pointCollection.is2_calendar\n",
    "\n",
    "import dask\n",
    "import dask.array\n",
    "import holoviews as hv\n",
    "import hvplot.dask\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "\n",
    "# import intake\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import pygmt\n",
    "import pyproj\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(n_workers=64, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = glob.glob(pathname=\"ATL11.001z123/ATL11_*.zarr\")\n",
    "print(f\"{len(stores)} reference ground track Zarr stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(\n",
    "    paths=stores,\n",
    "    group=\"pt123\",\n",
    "    engine=\"zarr\",\n",
    "    combine=\"nested\",\n",
    "    concat_dim=\"ref_pt\",\n",
    "    backend_kwargs={\"consolidated\": True},\n",
    ")\n",
    "# ds = ds.unify_chunks().compute()\n",
    "# TODO use intake\n",
    "# source = intake.open_ndzarr(url=\"ATL11.001z123/ATL11_0*.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot into a pandas/dask dataframe\n",
    "\n",
    "To make data analysis and plotting easier,\n",
    "let's flatten our n-dimensional xarray.Dataset\n",
    "to a 2-dimensiontal pandas.DataFrame table format.\n",
    "\n",
    "There are currently 6 cycles (as of March 2020),\n",
    "and by selecting just one cycle at a time,\n",
    "we can see what the height (h_corr)\n",
    "of the ice is like at that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at ICESat-2 Cycle 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to needed columns\n",
    "dss = ds.sel(cycle_number=6)[\n",
    "    [\"longitude\", \"latitude\", \"h_corr\", \"delta_time\", \"cycle_number\"]\n",
    "]\n",
    "dss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = hv.Points(\n",
    "    data=dss.set_coords([\"longitude\", \"latitude\"]),\n",
    "    label=\"Cycle_6\",\n",
    "    kdims=[\"longitude\", \"latitude\"],\n",
    "    vdims=[\"h_corr\", \"delta_time\", \"cycle_number\"],\n",
    "    datatype=[\"xarray\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = points.dframe()  # convert to pandas.DataFrame, slow\n",
    "df = df.dropna()  # drop empty rows\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert geographic lon/lat to x/y\n",
    "\n",
    "To center our plot on the South Pole,\n",
    "we'll reproject the original longitude/latitude coordinates\n",
    "to the Antarctic Polar Stereographic (EPSG:3031) projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "x, y = pyproj.Proj(projparams=3031)(df.longitude.values, df.latitude.values)\n",
    "df[\"x\"], df[\"y\"] = x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert delta_time to time\n",
    "\n",
    "To get more human-readable datetimes,\n",
    "we'll convert the delta_time attribute from the original GPS time format\n",
    "(nanoseconds since the beginning of ICESat-2 starting epoch)\n",
    "to Coordinated Universal Time (UTC).\n",
    "The reference date for the ICESat-2 Epoch is 2018 January 1st according to\n",
    "https://github.com/SmithB/pointCollection/blob/master/is2_calendar.py#L11-L15\n",
    "\n",
    "TODO: Account for [leap seconds](https://en.wikipedia.org/wiki/Leap_second)\n",
    "in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICESAT2_EPOCH = pointCollection.is2_calendar.t_0()\n",
    "# ICESAT2_EPOCH = datetime.datetime(2018, 1, 1, 0, 0, 0)\n",
    "df[\"time\"] = ICESAT2_EPOCH + df.delta_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a sample of the points\n",
    "\n",
    "Let's take a look at an interactive map\n",
    "of the ICESat-2 ATL11 height for Cycle 6!\n",
    "We'll plot a random sample (n=5 million)\n",
    "of the points instead of the whole dataset,\n",
    "it should give a good enough picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=5_000_000).hvplot.points(\n",
    "    x=\"x\", y=\"y\", c=\"h_corr\", cmap=\"Blues\", rasterize=True, hover=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "sorted(os.listdir(\"ATL11.001/\"))\n",
    "thefile = \"ATL11.001/ATL11_076211_0104_02_v001.h5\"\n",
    "\n",
    "with h5py.File(thefile, mode=\"r\") as h5f:\n",
    "    print(h5f.keys())\n",
    "    print(h5f[\"pt1\"].keys())\n",
    "\n",
    "with xr.open_dataset(thefile, group=\"pt1/ref_surf\", engine=\"h5netcdf\") as rs:\n",
    "    # read in the along-track coordinates\n",
    "    x_atc = rs.x_atc.to_masked_array()\n",
    "    # N slope\n",
    "    n_slope = rs.n_slope.to_masked_array()\n",
    "    e_slope = rs.e_slope.to_masked_array()\n",
    "    bad = n_slope == 1.7976931348623157e308\n",
    "    bad |= e_slope == 1.7976931348623157e308\n",
    "    n_slope[bad] = np.NaN\n",
    "    e_slope[bad] = np.NaN\n",
    "    slope_mag = np.sqrt(n_slope ** 2 + e_slope ** 2)\n",
    "    # get the reference-surface quality summary\n",
    "    r_quality_summary = rs.quality_summary.to_masked_array()\n",
    "\n",
    "with xr.open_dataset(thefile, group=\"pt1/corrected_h\", engine=\"h5netcdf\") as ch:\n",
    "    # read in the along-track coordinates\n",
    "    ref_pt = ch.ref_pt.to_masked_array()\n",
    "    # read in the corrected_h\n",
    "    h_corr = ch.h_corr.to_masked_array()\n",
    "    # mask out invalid values\n",
    "    bad = h_corr == 1.7976931348623157e308\n",
    "    h_corr[bad] = np.NaN\n",
    "\n",
    "    # error\n",
    "    h_corr_sigma = ch.h_corr_sigma.to_masked_array()\n",
    "    bad = h_corr_sigma == 1.7976931348623157e308\n",
    "    h_corr_sigma[bad] = np.NaN\n",
    "    # systematic error\n",
    "    h_corr_sigma_s = ch.h_corr_sigma_systematic.to_masked_array()\n",
    "    bad = h_corr_sigma == 1.7976931348623157e308\n",
    "    h_corr_sigma_s[bad] = np.NaN\n",
    "    # get the ATL06-based quality summary\n",
    "    h_quality_summary = ch.quality_summary.to_masked_array()\n",
    "    # read the cycle_number\n",
    "    cycle_num = ch.cycle_number.to_masked_array()\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    plt.figure()\n",
    "    plt.subplot(211)\n",
    "    for cycle in range(0, h_corr.shape[1]):\n",
    "        plt.plot(x_atc, h_corr[:, cycle], \".\", label=f\"cycle {cycle}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"along-track distance\")\n",
    "    plt.ylabel(\"height, m\")\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(x_atc, np.sum(np.isfinite(h_corr), axis=1), \".\")\n",
    "    plt.xlabel(\"along-track x\")\n",
    "    plt.ylabel(\"number of cycles present\")\n",
    "\n",
    "x_rep = np.tile(x_atc[:, np.newaxis], [1, len(cycle_num)])\n",
    "q_rep = np.tile(r_quality_summary[:, np.newaxis], [1, len(cycle_num)])\n",
    "\n",
    "if 2 == 2:\n",
    "    plt.figure()\n",
    "    plt.plot(x_atc, r_quality_summary, \".\")\n",
    "    plt.xlabel(\"x_atc\")\n",
    "    plt.ylabel(\"reference-surface quality summary\")\n",
    "\n",
    "\n",
    "if 3 == 3:\n",
    "    fig = plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.plot(\n",
    "        x_rep.ravel()[q_rep.ravel() != 6], h_corr.ravel()[q_rep.ravel() != 6], \"k.\"\n",
    "    )\n",
    "    plt.title(\"points with surface quality not equal to 6\")\n",
    "    # plt.gca().set_xlim([6.75e6, 6.87e6])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 4 == 4:\n",
    "    comblist = list(itertools.combinations(cycle_num, 2))\n",
    "    plt.figure(len(comblist) + 1, figsize=(4, 2 * len(comblist) + 1))\n",
    "    plt.clf()\n",
    "    ax = []\n",
    "    good = np.flatnonzero(q_rep[:, 0] != 6)\n",
    "\n",
    "    # cycle-to-cycle elevation differences\n",
    "    for j, (col1, col2) in enumerate(comblist, start=1):\n",
    "        ax += [plt.subplot(len(comblist) + 1, 1, j)]\n",
    "        col1 -= 1\n",
    "        col2 -= 1\n",
    "\n",
    "        this_dh = h_corr[good, col2] - h_corr[good, col1]\n",
    "        # cycle-to-cycle difference errors are the quadratic sums of the cycle errors\n",
    "        this_dh_sigma = np.sqrt(\n",
    "            h_corr_sigma[good, col2] ** 2 + h_corr_sigma[good, col1] ** 2\n",
    "        )\n",
    "        # Likewise for systematic errors:\n",
    "        this_dh_sigma_s = np.sqrt(\n",
    "            h_corr_sigma_s[good, col2] ** 2 + h_corr_sigma_s[good, col1] ** 2\n",
    "        )\n",
    "        plt.errorbar(\n",
    "            x_rep[good, col2].ravel(),\n",
    "            this_dh,\n",
    "            yerr=np.sqrt(this_dh_sigma ** 2 + this_dh_sigma_s ** 2),\n",
    "            fmt=\"r.\",\n",
    "        )\n",
    "        plt.errorbar(x_rep[good, col2].ravel(), this_dh, yerr=this_dh_sigma, fmt=\"k.\")\n",
    "        ax[-1].set_ylabel(f\"cycle {col2+1} \\n minus \\n cycle {col1+1}\")\n",
    "        ax[-1].set_ylim([-5, 5])\n",
    "\n",
    "    # plot of the number of cycles available:\n",
    "    ax += [plt.subplot(len(comblist) + 1, 1, j + 1)]\n",
    "    plt.plot(x_atc, np.sum(np.isfinite(h_corr) & (q_rep != 6), axis=1), \".\")\n",
    "    ax[-1].set_ylabel(\"# of cycles available\")\n",
    "    ax[-1].set_ylim([0, 3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "dh (change in elevation) over Antarctica!"
   },
   "outputs": [],
   "source": [
    "def read_field(dataset: xr.Dataset, field: str):\n",
    "    data = dataset[field].to_masked_array()\n",
    "    bad1 = data == 1.7976931348623157e308\n",
    "    data[bad1] = np.NaN\n",
    "    bad2 = data == -1.7976931348623157e308\n",
    "    data[bad2] = np.NaN\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_ATL11(filepath: str, pair: str = \"pt2\", epsg: int = 3031):\n",
    "    with xr.open_mfdataset(\n",
    "        paths=filepath, group=f\"{pair}/corrected_h\", engine=\"h5netcdf\"\n",
    "    ) as ch:\n",
    "        longitude = read_field(dataset=ch, field=\"longitude\")\n",
    "        latitude = read_field(dataset=ch, field=\"latitude\")\n",
    "        h_corr = read_field(dataset=ch, field=\"h_corr\")\n",
    "        h_corr_sigma = read_field(dataset=ch, field=\"h_corr_sigma\")\n",
    "        h_corr_sigma_s = read_field(dataset=ch, field=\"h_corr_sigma_systematic\")\n",
    "    with xr.open_mfdataset(\n",
    "        paths=filepath, group=f\"{pair}/ref_surf\", engine=\"h5netcdf\"\n",
    "    ) as rs:\n",
    "        x_atc = read_field(dataset=rs, field=\"x_atc\")\n",
    "        quality = rs[\"quality_summary\"].to_masked_array().data\n",
    "    h_corr[quality == 6] = np.NaN\n",
    "    x, y = pyproj.Proj(projparams=epsg)(longitude, latitude)\n",
    "    return x_atc, x, y, h_corr, np.sqrt(h_corr_sigma ** 2 + h_corr_sigma_s ** 2)\n",
    "\n",
    "\n",
    "x_atc = []\n",
    "x = []\n",
    "y = []\n",
    "h_corr = []\n",
    "sigma_h = []\n",
    "for pair in [\"pt1\", \"pt2\", \"pt3\"]:\n",
    "    xx_atc, xx, yy, hh, ss = read_ATL11(\n",
    "        filepath=\"ATL11.001/ATL11_????11_0105_02_v001.h5\", pair=pair\n",
    "    )\n",
    "    x_atc += [xx_atc]\n",
    "    x += [xx]\n",
    "    y += [yy]\n",
    "    h_corr += [hh]\n",
    "    sigma_h += [ss]\n",
    "\n",
    "# with xr.open_mfdataset(\n",
    "#     paths=\"ATL11.001/ATL11_????11_0105_02_v001.h5\",\n",
    "#     group=f\"{pair}/corrected_h\",\n",
    "#     engine=\"h5netcdf\",\n",
    "#     lock=False,\n",
    "#     # combine=\"nested\",\n",
    "#     # concat_dim=None,\n",
    "#     # drop_variables=\"delta_time\"\n",
    "# ) as ch:\n",
    "#     pass\n",
    "\n",
    "x_atc = np.concatenate(x_atc)\n",
    "x = np.concatenate(x)\n",
    "y = np.concatenate(y)\n",
    "h_corr = np.concatenate(h_corr, axis=0)\n",
    "sigma_h = np.concatenate(sigma_h, axis=0)\n",
    "\n",
    "print(h_corr.shape)\n",
    "\n",
    "if 5 == 5:\n",
    "    c2: int = 5\n",
    "    c1: int = 4\n",
    "    plt.figure(figsize=[10, 10])\n",
    "    plt.scatter(\n",
    "        x=x[::20],\n",
    "        y=y[::20],\n",
    "        s=2,\n",
    "        c=h_corr[::20, c2 - 1] - h_corr[::20, c1 - 1],\n",
    "        vmin=-2,\n",
    "        vmax=2,\n",
    "        cmap=\"Spectral\",\n",
    "    )\n",
    "    plt.axis(\"equal\")\n",
    "    hb = plt.colorbar()\n",
    "    hb.set_label(f\"cycle {c2} minus cycle {c1} elevation change (dh) in metres\")\n",
    "\n",
    "# TODO https://github.com/ICESAT-2HackWeek/elevation-change/blob/master/elevation_change_with_ATL11.ipynb\n",
    "\n",
    "sdf = pd.DataFrame(sigma_h, columns=[f\"s{i + 1}\" for i in range(5)])\n",
    "\n",
    "df = pd.concat(\n",
    "    objs=[\n",
    "        pd.DataFrame(data=x_atc, columns=[\"x_atc\"]),\n",
    "        pd.DataFrame(data=x, columns=[\"x\"]),\n",
    "        pd.DataFrame(data=y, columns=[\"y\"]),\n",
    "        pd.DataFrame(data=h_corr, columns=[f\"h{i + 1}\" for i in range(5)]),\n",
    "    ],\n",
    "    axis=\"columns\",\n",
    ")\n",
    "df.head()\n",
    "\n",
    "# TODO range of dh along window view of point with big change\n",
    "\n",
    "# Cycle 1 - Spring2018 - 13Oct2018 - 28Dec2018  -ve MassBalance\n",
    "# Cycle 2 - Summer2019 - 28Dec2018 - 29Mar2019 --ve MassBalance\n",
    "# Cycle 3 - Autumn2019 - 29Mar2019 - 28Jun2019  +ve MassBalance *\n",
    "# Cycle 4 - Winter2019 - 09Jul2019 - 26Sep2019 ++ve MassBalance *\n",
    "# Cycle 5 - Spring2019 - 26Sep2019 - 26Dec2019  -ve MassBalance *\n",
    "# Cycle 6 - Summer2020 - 26Dec2019 - 26Mar2020 --ve MassBalance\n",
    "\n",
    "hmin = df[[f\"h{i+1}\" for i in range(5)]].min(axis=\"columns\")  # minimum elevation\n",
    "hmax = df[[f\"h{i+1}\" for i in range(5)]].max(axis=\"columns\")  # maximum elevation\n",
    "df[\"hrange\"] = hmax - hmin  # range of elevation across all cycles\n",
    "df.hrange.replace(to_replace=0.0, value=np.NaN, inplace=True)\n",
    "df.to_csv(\"xyhr.csv\")\n",
    "# df = pd.read_csv(\"xyhr.csv\", index_col=0)\n",
    "bigdh = df[df[\"hrange\"] > 5.5]  # find points where elevation range is greater than 5.5m\n",
    "bigdh\n",
    "bigdh.index\n",
    "\n",
    "# TODO point in polygon (grounding line) to filter out ice shelf dynamics\n",
    "\n",
    "for i in bigdh.index[:]:\n",
    "    # i = 4848718\n",
    "    temp_df = df.loc[i - 10 : i + 10]\n",
    "    median_change = temp_df.hrange.median()\n",
    "    if median_change >= 5.5 and median_change < 50:\n",
    "        temp_sdf = sdf.loc[i - 10 : i + 10]\n",
    "        for j in range(5):\n",
    "            plt.errorbar(\n",
    "                x=temp_df.x_atc,\n",
    "                y=temp_df[f\"h{j+1}\"],\n",
    "                yerr=temp_sdf[f\"s{j+1}\"],\n",
    "                fmt=\"k.\",\n",
    "            )\n",
    "            plt.scatter(x=temp_df.x_atc, y=temp_df[f\"h{j+1}\"], label=f\"h{j+1}\")\n",
    "        plt.title(\n",
    "            label=f\"xy:{temp_df.loc[i].x},{temp_df.loc[i].y}\\nindex:{i}, median_change:{median_change}m\"\n",
    "        )\n",
    "\n",
    "        plt.gca().set_xlim(temp_df.x_atc[i - 10], temp_df.x_atc[i + 10])\n",
    "        # plt.gca().set_ylim(160, 200)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Subglacial lake Slessor2 uplift\n",
    "# -410918.8386,1029347.4666\n",
    "# -408131.9125,1031128.9651\n",
    "# Subglacial Lake Slessor4 drain\n",
    "# -338117.9641,1110603.6373\n",
    "\n",
    "# Subglacial Lake Whillans4/Mercer2 drainage\n",
    "# -307154.8016,-507734.7378\n",
    "\n",
    "# Subglacial Lake Macayeal 3 drainage (manually found)\n",
    "# -734532.7023, -855436.2967\n",
    "\n",
    "# Subglacial Lake Byrd 2 uplift (manually found, ~2m)\n",
    "# 557187.1725,-855601.0561\n",
    "# 555843.4189,-985710.3337 # Upstream Byrd Glacier ? rifting ??\n",
    "\n",
    "# -741220.3139, 937483.8670 # Ronne-Filchner Ice Shelf\n",
    "# -973351.7558, 272566.6157 # Ronne-Filchner Ice Shelf\n",
    "# -1008445.1929,274272.3455  # Ronne-Filchner Ice Shelf\n",
    "# 37261.1917,-1180880.8635 Ross Sea tidal motion\n",
    "# -579964.1805,574791.5220 # Support Force Glacier at grounding line\n",
    "# -1174079.4108,212533.0448 # Rutford Ice Stream/Shelf tidal motion?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "deepicedrain",
   "language": "python",
   "name": "deepicedrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
